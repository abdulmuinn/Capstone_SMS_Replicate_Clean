{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497fb450",
   "metadata": {},
   "source": [
    "# ðŸ¤–ðŸ“Š Capstone: Public Data Analytics with Replicate (SMS Spam)\n",
    "**Tujuan:** Menunjukkan alur *endâ€‘toâ€‘end* analisis data publik dengan bantuan LLM di **Replicate API**.  \n",
    "Dataset: **SMS Spam Collection (5000 rows)** â€” kolom: `label` (`ham`/`spam`), `message` (teks).\n",
    "\n",
    "### Output Akhir\n",
    "- `predictions.csv` â€” data + label hasil zeroâ€‘shot (jika dijalankan)\n",
    "- `report.md` â€” *Analytical Result*, *Insights & Findings*, *Recommendations* (Markdown)\n",
    "\n",
    "> Jalankan sel **berurutan dari atas ke bawah**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c490f0",
   "metadata": {},
   "source": [
    "## Daftar Isi\n",
    "1. [Prasyarat & Token](#1-Prasyarat--Token)\n",
    "2. [Konfigurasi Proyek](#2-Konfigurasi-Proyek)\n",
    "3. [Load Dataset](#3-Load-Dataset)\n",
    "4. [EDA Singkat](#4-EDA-Singkat)\n",
    "5. [Preprocessing Teks](#5-Preprocessing-Teks)\n",
    "6. [Zeroâ€‘Shot Classification (LLM)](#6-ZeroShot-Classification-LLM)\n",
    "7. [Baseline Supervised (TFâ€‘IDF + LR)](#7-Baseline-Supervised-TFIDF--LR)\n",
    "8. [Generate Insight & Rekomendasi](#8-Generate-Insight--Rekomendasi)\n",
    "9. [Simpan Output](#9-Simpan-Output)\n",
    "10. [Catatan & Tips](#10-Catatan--Tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c0cd3d",
   "metadata": {},
   "source": [
    "## 1) Prasyarat & Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e9ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependensi\n",
    "!pip -q install replicate pandas numpy scikit-learn matplotlib tqdm\n",
    "\n",
    "# Set & cek token Replicate\n",
    "# >>> GANTI 'xxxxxxxx' dengan token aslimu atau gunakan: %env REPLICATE_API_TOKEN=... <<<\n",
    "import os\n",
    "os.environ.setdefault(\"REPLICATE_API_TOKEN\", \"xxxxxxxxxxxxxxxx\")\n",
    "\n",
    "print(\"Token terdeteksi:\", bool(os.getenv(\"REPLICATE_API_TOKEN\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5edd4e",
   "metadata": {},
   "source": [
    "## 2) Konfigurasi Proyek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965a5ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import umum & konfigurasi\n",
    "import os, json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.rcParams.update({\"figure.figsize\": (10,5), \"axes.grid\": True})\n",
    "\n",
    "# Model LLM di Replicate yang akan dipakai\n",
    "MODEL_ID = \"meta/meta-llama-3-8b-instruct\"   # ubah sesuai model yang tersedia di akunmu\n",
    "\n",
    "# Skema label untuk zero-shot\n",
    "LABELS = [\"spam\", \"ham\"]\n",
    "\n",
    "# Sampling & batch (menjaga biaya/token)\n",
    "SAMPLE_N = 200     # jumlah sample text untuk zero-shot\n",
    "BATCH    = 25      # ukuran batch per panggilan LLM\n",
    "\n",
    "print(\"Konfigurasi siap. MODEL_ID:\", MODEL_ID, \"| LABELS:\", LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f93d9ce",
   "metadata": {},
   "source": [
    "## 3) Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e206a29",
   "metadata": {},
   "source": [
    "Upload file `sms_spam_full.csv` (5000 baris) ke panel **Files** di Colab, lalu jalankan sel di bawah.  \n",
    "Jika belum punya filenya, unduh dari chat, atau ganti nama file sesuai lokasi Anda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ccf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"sms_spam_full.csv\"  # ganti jika nama file berbeda\n",
    "\n",
    "# Load aman dengan validasi sederhana\n",
    "try:\n",
    "    df = pd.read_csv(DATA_FILE)\n",
    "except FileNotFoundError as e:\n",
    "    raise FileNotFoundError(f\"Tidak menemukan file '{DATA_FILE}'. Pastikan sudah upload ke Colab Files.\") from e\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "display(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e81955",
   "metadata": {},
   "source": [
    "## 4) EDA Singkat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b22940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Info dataset:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nMissing values per kolom:\")\n",
    "display(df.isna().sum().to_frame(\"n_missing\").sort_values(\"n_missing\", ascending=False))\n",
    "\n",
    "print(\"\\nDistribusi label:\")\n",
    "if \"label\" in df.columns:\n",
    "    display(df[\"label\"].value_counts())\n",
    "else:\n",
    "    print(\"Kolom 'label' tidak ada (tidak masalah jika hanya ingin zero-shot).\")\n",
    "\n",
    "print(\"\\nContoh 5 baris:\")\n",
    "display(df.sample(5, random_state=42))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e455d3d5",
   "metadata": {},
   "source": [
    "## 5) Preprocessing Teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e054f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = str(s)\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"http\\S+|www\\S+\", \" \", s)        # hapus URL\n",
    "    s = re.sub(r\"[^0-9a-zA-Z\\s]+\", \" \", s)        # hapus simbol\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()            # normalisasi spasi\n",
    "    return s\n",
    "\n",
    "if \"message\" not in df.columns:\n",
    "    raise KeyError(\"Kolom 'message' tidak ditemukan. Pastikan dataset punya kolom 'message'.\")\n",
    "\n",
    "df[\"_text_clean\"] = df[\"message\"].apply(clean_text)\n",
    "display(df[[\"message\",\"_text_clean\"]].head(5))\n",
    "\n",
    "# Word frequency plot (opsional)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer(max_features=2000, stop_words=\"english\")\n",
    "X_counts = vec.fit_transform(df[\"_text_clean\"].fillna(\"\"))\n",
    "word_sums = np.asarray(X_counts.sum(axis=0)).ravel()\n",
    "vocab = np.array(vec.get_feature_names_out())\n",
    "\n",
    "top_n = 20\n",
    "idx = word_sums.argsort()[::-1][:top_n]\n",
    "top_words = vocab[idx]\n",
    "top_freqs = word_sums[idx]\n",
    "\n",
    "plt.bar(range(len(top_words)), top_freqs)\n",
    "plt.xticks(range(len(top_words)), top_words, rotation=80)\n",
    "plt.title(\"Top Word Frequencies (Cleaned)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23186eb",
   "metadata": {},
   "source": [
    "## 6) ZeroShot Classification (LLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f4b322",
   "metadata": {},
   "source": [
    "**Penjelasan singkat:**  \n",
    "- Kita minta LLM mengklasifikasikan teks tanpa contoh (*zeroâ€‘shot*) menjadi label di `LABELS` (`spam`/`ham`).  \n",
    "- Prompt diatur *JSON-only* supaya hasil mudah di-parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2271a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n",
    "\n",
    "REPLICATE_API_TOKEN = os.getenv(\"REPLICATE_API_TOKEN\", \"\")\n",
    "if not REPLICATE_API_TOKEN:\n",
    "    raise EnvironmentError(\"REPLICATE_API_TOKEN belum di-set. Lihat bagian 'Prasyarat & Token'.\")\n",
    "\n",
    "def replicate_generate(prompt: str, model_id: str = MODEL_ID, **extra):\n",
    "    \"\"\"Panggil model text-generation di Replicate.\n",
    "    Mencoba beberapa kunci input umum: 'prompt', 'input', 'text'.\n",
    "    Mengembalikan string gabungan.\n",
    "    \"\"\"\n",
    "    try_inputs = [\n",
    "        {\"prompt\": prompt, **extra},\n",
    "        {\"input\": prompt,  **extra},\n",
    "        {\"text\":  prompt,  **extra},\n",
    "    ]\n",
    "    last_err = None\n",
    "    for payload in try_inputs:\n",
    "        try:\n",
    "            out = replicate.run(model_id, input=payload)\n",
    "            if isinstance(out, str):\n",
    "                return out\n",
    "            if hasattr(out, \"__iter__\"):\n",
    "                return \"\".join([str(x) for x in out])\n",
    "            return json.dumps(out, ensure_ascii=False)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise RuntimeError(f\"Replicate error: {last_err}\")\n",
    "\n",
    "# Sampling supaya hemat token\n",
    "texts = df[\"_text_clean\"].dropna().astype(str).tolist()[:SAMPLE_N]\n",
    "\n",
    "def zero_shot_batch(batch_texts, labels):\n",
    "    sys = (\n",
    "        \"You are a strict JSON-only classifier. \"\n",
    "        f\"Classify each line into exactly one of: {labels}. \"\n",
    "        \"Return ONLY a valid JSON array of objects with keys: 'text' and 'label'. No explanations.\"\n",
    "    )\n",
    "    numbered = \"\\n\".join([f\"{i+1}. {t}\" for i, t in enumerate(batch_texts)])\n",
    "    prompt = f\"\"\"{sys}\n",
    "\n",
    "Lines:\n",
    "{numbered}\n",
    "\"\"\"\n",
    "    raw = replicate_generate(prompt)\n",
    "    # Parse JSON robust\n",
    "    try:\n",
    "        data = json.loads(raw)\n",
    "    except Exception:\n",
    "        m = re.search(r\"(\\[.*\\])\", raw, flags=re.S)\n",
    "        if m:\n",
    "            data = json.loads(m.group(1))\n",
    "        else:\n",
    "            # fallback: kembalikan struktur minimal\n",
    "            data = [{\"text\": t, \"label\": None, \"raw\": raw} for t in batch_texts]\n",
    "    return data\n",
    "\n",
    "# Jalankan per-batch\n",
    "zs_rows = []\n",
    "for i in tqdm(range(0, len(texts), BATCH)):\n",
    "    part = texts[i:i+BATCH]\n",
    "    zs_rows.extend(zero_shot_batch(part, LABELS))\n",
    "\n",
    "zs_df = pd.DataFrame(zs_rows)\n",
    "display(zs_df.head())\n",
    "\n",
    "# Gabungkan kembali ke df\n",
    "if not zs_df.empty and \"label\" in zs_df.columns:\n",
    "    df = df.merge(\n",
    "        zs_df[[\"text\",\"label\"]].rename(columns={\"text\":\"_text_clean\",\"label\":\"_label_zs\"}),\n",
    "        on=\"_text_clean\", how=\"left\"\n",
    "    )\n",
    "    print(\"Distribusi label (zeroâ€‘shot):\")\n",
    "    display(df[\"_label_zs\"].value_counts(dropna=False))\n",
    "else:\n",
    "    print(\"Tidak ada hasil zeroâ€‘shot yang bisa digabungkan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfc0057",
   "metadata": {},
   "source": [
    "## 7) Baseline Supervised (TFIDF + LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96610cf5",
   "metadata": {},
   "source": [
    "**Tujuan:** Membuat baseline cepat & interpretabel.  \n",
    "- Fitur: **TFâ€‘IDF (1â€“2 gram)**  \n",
    "- Model: **Logistic Regression**  \n",
    "- Target: hasil zeroâ€‘shot (`_label_zs`) *atau* label asli (`label`) bila tersedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cbcdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "target_series = None\n",
    "if \"_label_zs\" in df.columns and df[\"_label_zs\"].notna().sum() > 0:\n",
    "    target_series = df[\"_label_zs\"]\n",
    "elif \"label\" in df.columns:\n",
    "    target_series = df[\"label\"]\n",
    "\n",
    "if target_series is None:\n",
    "    print(\"Lewati: belum ada target label (zeroâ€‘shot atau label asli).\")\n",
    "else:\n",
    "    sup = df.dropna(subset=[\"_text_clean\"]).copy()\n",
    "    sup = sup[target_series.notna()]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        sup[\"_text_clean\"].values, target_series.loc[sup.index].values,\n",
    "        test_size=0.2, random_state=42, stratify=target_series.loc[sup.index].values\n",
    "    )\n",
    "\n",
    "    vec = TfidfVectorizer(max_features=3000, ngram_range=(1,2))\n",
    "    Xtr = vec.fit_transform(X_train)\n",
    "    Xte = vec.transform(X_test)\n",
    "\n",
    "    clf = LogisticRegression(max_iter=200)\n",
    "    clf.fit(Xtr, y_train)\n",
    "    pred = clf.predict(Xte)\n",
    "\n",
    "    print(classification_report(y_test, pred))\n",
    "\n",
    "    cm = confusion_matrix(y_test, pred, labels=sorted(list(set(y_test))))\n",
    "    plt.imshow(cm, aspect=\"auto\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xlabel(\"Pred\")\n",
    "    plt.colorbar()\n",
    "    plt.xticks(range(len(set(y_test))), sorted(list(set(y_test))), rotation=45)\n",
    "    plt.yticks(range(len(set(y_test))), sorted(list(set(y_test))))\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8607fd72",
   "metadata": {},
   "source": [
    "## 8) Generate Insight & Rekomendasi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09ab3d4",
   "metadata": {},
   "source": [
    "Menggunakan LLM untuk menulis ringkasan analitik dalam **Bahasa Indonesia**.  \n",
    "Struktur keluaran: *Analytical Result*, *Insight & Findings*, *Recommendations*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf91b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {\n",
    "    \"n_rows\": int(df.shape[0]),\n",
    "    \"n_cols\": int(df.shape[1]),\n",
    "}\n",
    "if \"_label_zs\" in df.columns:\n",
    "    stats[\"label_counts_zs\"] = df[\"_label_zs\"].value_counts(dropna=False).to_dict()\n",
    "if \"label\" in df.columns:\n",
    "    stats[\"label_counts_original\"] = df[\"label\"].value_counts(dropna=False).to_dict()\n",
    "\n",
    "prompt = f\"\"\"Kamu adalah data analyst senior. Berdasarkan statistik/temuan berikut (format JSON):\n",
    "{json.dumps(stats, ensure_ascii=False, indent=2)}\n",
    "\n",
    "Tulis output dalam Bahasa Indonesia dengan format Markdown dan struktur:\n",
    "### Analytical Result\n",
    "- (3â€“5 bullet)\n",
    "\n",
    "### Insight & Findings\n",
    "- (3â€“5 bullet)\n",
    "\n",
    "### Recommendations\n",
    "- (3â€“5 bullet, actionable, prioritas jangka pendek & panjang)\n",
    "\"\"\"\n",
    "\n",
    "insight_md = replicate_generate(prompt)\n",
    "print(insight_md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5605103b",
   "metadata": {},
   "source": [
    "## 9) Simpan Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05de839",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_files = []\n",
    "\n",
    "if \"_label_zs\" in df.columns:\n",
    "    df.to_csv(\"predictions.csv\", index=False)\n",
    "    out_files.append(\"predictions.csv\")\n",
    "\n",
    "try:\n",
    "    _ = insight_md  # noqa\n",
    "    with open(\"report.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(insight_md)\n",
    "    out_files.append(\"report.md\")\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "print(\"Saved:\", out_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9f70f5",
   "metadata": {},
   "source": [
    "## 10) Catatan & Tips\n",
    "- **Kontrol biaya/token**: atur `SAMPLE_N` dan `BATCH`.\n",
    "- **Label set** harus **mutually exclusive** (tidak tumpang tindih).\n",
    "- Jika dataset **nonâ€‘teks**, fokus pada EDA & gunakan LLM untuk **narasi insight** saja.\n",
    "- Baseline bisa ditingkatkan: ubah nâ€‘gram, tambah `max_features`, atau coba model lain (LinearSVC, etc.).\n",
    "- Dokumentasikan alasan teknis & tradeâ€‘off (akurasi vs biaya, interpretabilitas, dsb.)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
